<!DOCTYPE html>
<html>
<head>
<title>AI</title>
<link rel="stylesheet" type="text/css" href="style.css">
<link rel="stylesheet" type="text/css" href="2.css">
<link rel="stylesheet" type="text/css" href="3.css">
<meta charset="utf-8"/>
<link rel="icon" href="favicon.ico" />
</head>

<body>

	<ul class="navbar">
		<li>
			<a href="indexE.html">A.I</a>
		</li>


		<li>
			<a href="1E.html">A brief chronology</a>
		</li>


		<li>
			<a href="2E.html">Benefits</a>
		</li>


		<li>
			<a href="3E.html">Limitations and dangers</a>
		</li>


		<li>
			<a href="4E.html">Ethical and legal issues</a>
		</li>


		<li>
			<a href="5E.html">Conclusion</a>
		</li>


		<li style="float:right;">
			<a href="../5.html">FR</a>
		</li>
	</ul>


<div id="menu">
		<ul>
			<li><button><span class="circle"></span> <span class="title"><a href="indexE.html">Introduction</a></span></button>
			</li>


			<li><button><span class="circle"></span> <span class="title"><a href="1E.html">A brief chronology</a></span></button>
			</li>


			<li><button><span class="circle"></span> <span class="title"><a href="2E.html">Benefits of AI</a></span></button>
			</li>


			<li><button><span class="circle"></span> <span class="title"><a href="3E.html">Limitations and dangers of AI</a></span></button>
			</li>


			<li><button><span class="circle"></span> <span class="title"><a href="4E.html">Ethical and legal issues</a></span></button>
			</li>


			<li><button><span class="circle"></span> <span class="title"><a href="5E.html">Conclusion</a></span></button>
			</li>
		</ul>
	</div>


<div class="in">
  <p style="background-color: white;font-size:12px;"><a href="indexE.html">Home</a> > Conclusion</a></p>
</div>		

<div class="in">
	<h1 class="ex">Conclusion</h1>		
	<p class="out">
		In the future, robots will be more and more present and will help us to accomplish more and more tasks, but the risks will be increased too, hence the need for a legal framework that strictly defines the limits not to exceed.<br>
		
		<img src="imgs/conclusion/hawking.jpg" class="notin" style="width:50%">

		Some scientists continue to view AI as a potential hazard. According to the mathematician and physicist Stephen Hawking "humans, limited by their slow biological evolution, would soon be replaced because it is impossible to compete against AI. ". In short, he announces the end of humanity. <br> <br>

This is also the case of Nick Bostrom, Swedish philosopher, who published in 2014 'Superintelligence'. According to him, the hypothesis of an AI capable of competing with human intelligence is still topical: "We do not know when artificial intelligence (AI) will be able to compete with human intelligence, but it will certainly happen in the twenty-first century »<br>
		
		<img src="imgs/conclusion/superintelligence.png" class="notin" style="width:50%">

		Nick Bostrom's research presupposes that two conditions are met to witness the takeover of intelligence on earth by machines. The first would be the existence of many low cost computers carrying this superintelligence, so that the computer population exceeds the human population. The second, called recursive self-improvement, means that software can improve itself.

Other researchers consider that we are far from the existence of extremely intelligent machines and that humanity is not in danger, as Stéphan Clémençon, pilot of the Machine learning for Big Data chair at Télécom ParisTech: <br> br>

"For the moment, there are few truly autonomous systems and the only negative points of machine learning, the capabilities of single-machine machines, are the destruction of jobs that automation processes generate. Given the state of the art in machine learning, there are still major steps to take before creating algorithms that can adapt to their environment alone. "<br> <br>

According to Thierry Artières, researcher at the laboratory of fundamental computer science of the University of Aix-Marseille, we are far from the existence of extremely intelligent machines: "We know how to improve software performance in confined perimeters, such as the recognition of people in images or the search for information ". For the moment, there is no machine capable of "decompartmentalizing these perimeters". <br>
		<img src="imgs/conclusion/gafam.png" class="notin" style="width:50%">

		The volume of data available to Google may seem worrying, but the researcher points out that "The advantage of Google, Facebook or Microsoft is not algorithmic but lies in the volume of data available: the more you have, the more you have reliable statistics that enhance the accuracy of your systems. "<br> <br>

So we are still far from being able to give birth to an artificial brain and for the moment humanity is not in danger. Even if a legal framework is essential and, as Seth Baum, executive director of Global catastrophic risk institute, says, "what is most important is that we take action now, to minimize the risk of a disaster afterwards ", in order to face the risks posed by the strong AI, there is no reason to be afraid of it. <br> <br>

	</p>
</div>

<div class="in">
			<div class="read">
				<a href="sourcesE.html">Sources <span class="arrow">&#8594;</span></a>
			</div>
</div>


<div id="footer">
	<div class="in">
		<a href="contactE.html">Contact</a>
		<a href="sourcesE.html">Sources</a><br>
		<a href="https://github.com/Weamix" target="_blank">© Tutored project at IUT Calais in 2018 done by Vitse Maxime, Goguillon Olivia and Dufour Victor</a>
	</div>
</div>

</body>
</html>